{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Apache Airflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. What is Apache Airflow?**\n",
    "\n",
    "Apache Airflow is a tool that helps automate tasks by organizing and scheduling them. It allows you to plan and execute tasks automatically according to a specific order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. DAGs**\n",
    "\n",
    "<img src=\"./images/dag_example.png\" width=\"700px\" />\n",
    "\n",
    "DAGs (Directed Acyclic Graphs) in Apache Airflow represent the tasks and their order of execution. Think of it as drawing a line from one point to another on a whiteboard, where each point represents a task.\n",
    "\n",
    "> Remember, DAGs are graph data structure that we've seen in a previous video (Data structures and algorithms with Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1. Operators**\n",
    "\n",
    "Operators in Apache Airflow are special tools used to perform different tasks. They are like superpowers that allow you to accomplish specific actions. For example, there are operators for sending emails, running commands, execute python codes, and more.\n",
    "\n",
    "> So, a task is created by using an operator and an operator is an abstraction that defines what has to be done in a task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2. Dependencies**\n",
    "\n",
    "Tasks are connected by dependencies indicating the order in which they should be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3. DAG schedule**\n",
    "\n",
    "It specifies when the DAG should be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4. Connectors to connect Apache Airflow to Cloud services: AWS, GCP, Azure**\n",
    "\n",
    "Apache Airflow can connect to cloud services like AWS, GCP, and Azure using connectors or hooks. These connectors are pieces of code that enable Airflow to communicate with the cloud services. By establishing connections and providing the necessary credentials, Airflow can interact with the cloud services and execute tasks on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Run Apache Airflow**\n",
    "\n",
    "* **This is the DAG we will create with Apache Airflow whose role is to orchestrate it (manage its execution).**\n",
    "\n",
    "<img src=\"./images/dag_example.png\" width=\"700px\" />\n",
    "\n",
    "* **For the transformation task with Python, we need the following libraries to installed:**\n",
    "\n",
    "> * Pandas\n",
    "> * Openpyxl\n",
    "> * xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. Create a Project's folder**\n",
    "\n",
    "**In this folder, we will build all our dags.**\n",
    "\n",
    "> * Ensure that Python is already installed on your computer\n",
    "> * Ensure that VS Code is already installed\n",
    "> * Ensure that WSL and Docker are already installed\n",
    "> * Ensure that in your VS code, you have installed Python extension and WSL extension\n",
    "> * Ensure that in your VS code, you have installed the microsoft official Docker extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Create a Dockerfile**\n",
    "\n",
    "* **Dockerfile: Definition (at the root of the project's folder')**\n",
    "\n",
    "> A Dockerfile is a text file containing a series of instructions for creating a Docker image. These instructions specify the various steps needed to configure the environment and dependencies required by an application or service inside a Docker container. The Dockerfile is used as input for the docker build command, which builds a Docker image from the instructions in the file.\n",
    "\n",
    "* **Utility Here**\n",
    "\n",
    "> The dockerfile will contain the instructions needed to build a custom image using the Airflow image as the base image, in which we'll also create a virtual python environment in which we'll install our dependencies (pandas, openpyxl, xlrd, ...).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Content of our Dockerfile**\n",
    "\n",
    "<img src=\"./images/dockerfile.png\" width=\"700px\" style=\"border:3px solid white\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Build the custom image using our Dockerfile**\n",
    "\n",
    "> **For this to work, you must open your Docker Desktop First**\n",
    "\n",
    "In VS Code:\n",
    "\n",
    "**Right-Click on the dockerfile and Click On build image**\n",
    "\n",
    "> **Give a name: let's say mds-airflow:latest**\n",
    "> > `mds-airflow` is the name\n",
    ">\n",
    "> > `latest` is the tag\n",
    "\n",
    "<img src=\"./images/dockerfile_exec.png\" width=\"400px\" />\n",
    "\n",
    "\n",
    "**A the end (installation is finished), tap enter in the terminal to exit and close the terminal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Create a docker-compose file**\n",
    "\n",
    "* **docker-compose: general definition**\n",
    "\n",
    "A Docker Compose file is a YAML configuration file used to define and manage multi-container Docker applications. It allows you to define the services, networks, volumes, and other configurations required to run and orchestrate multiple containers as a single application.\n",
    "\n",
    "> * **Services:** a service refers to a containerized application or component that is defined in the configuration file. It represents a specific task or functionality within the overall application architecture.\n",
    "> \n",
    "> * **Networks:** a network is a virtual network that allows communication between containers. It isolates and connects containers, enabling them to communicate with each other using container names as hostnames.\n",
    "> * **Volumes:** s volume is a persistent data storage mechanism in Docker Compose. It provides a way to store and share data between containers or between containers and the host machine. Volumes are used to persist data even when containers are stopped or removed, ensuring data durability and availability.\n",
    "\n",
    "* **docker-compose: definition in the context of Apache Airflow**\n",
    "\n",
    "In the context of Apache Airflow, a Docker Compose file can be used to define the various components and dependencies required to run an Airflow deployment. It enables you to specify the Airflow web server, scheduler, worker, and other services, as well as any necessary volumes, networks, and environment variables. With Docker Compose, you can easily define and manage a complete Airflow environment with multiple interconnected containers, simplifying the setup and deployment process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Content of our docker-compose file**\n",
    "\n",
    "In this, we will do two things:\n",
    "\n",
    "> * Expose port 8080 on which airflow runs to our localhost network traffic port 8080\n",
    ">\n",
    "> * For data persistency, we will create a volume to store data locally and bind the stored local db to a folder in our container\n",
    "\n",
    "<img src=\"./images/docker-compose_content.png\" width=\"600px\" style=\"border:3px solid white\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now, right-click on `docker-compose.yml`, then on `Compose Up`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4. Create neccessary folders for Airflow: `dags` and `plugins`**\n",
    "\n",
    "In Apache Airflow, the dags and plugins folders play important roles in organizing and managing your workflows and custom code.\n",
    "\n",
    "* **`dags` folder**\n",
    "\n",
    "> This is the default location where you store your DAG files. Airflow scans this folder to discover and schedule the defined DAGs. Any Python file placed in this folder (or its subdirectories) that contains a DAG definition will be automatically detected by Airflow.\n",
    "\n",
    "* **`plugins` folder**\n",
    "\n",
    "> This folder is used to store custom Airflow plugins. Plugins are reusable components that can extend the functionality of Airflow. They can include operators, sensors, hooks, macros, and more. By placing your custom plugin files in the plugins folder, Airflow will automatically load and make them available for use in your DAGs.\n",
    "\n",
    "Then:\n",
    "\n",
    "* **Check `airflow.cfg` file to ensure airflow will recognize `dags` and `plugins` folders**\n",
    "\n",
    "* **In the `dags` folder, create a subfolder named: `custom_modules`**\n",
    "\n",
    "This will contains our custom python codes (classes, functions, ...) to process data for example. These custom code could be called in our dags.\n",
    "\n",
    "> **Note:**\n",
    "> * When Airflow scans the dags directory for DAG files, it imports all Python files it encounters. During the import process, Airflow checks if the file contains a valid DAG definition. If a file defines a DAG using the DAG class from the Airflow library, it is considered a DAG file and will be scheduled and executed accordingly.\n",
    ">\n",
    "> * If a Python file within the dags directory does not define a DAG or use the DAG class, it will still be imported and executed during the DAG parsing process. However, it won't be treated as a DAG itself, and its functions or code can be used within DAG tasks or other Python files that are part of the DAG.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Create our First DAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1. Create a Python file named `weather_dag.py` and import the following libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from datetime import timedelta, datetime\n",
    "from airflow.providers.http.sensors.http import HttpSensor\n",
    "import json\n",
    "from airflow.providers.http.operators.http import SimpleHttpOperator\n",
    "from airflow.operators.python import PythonOperator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2. Define the Orchestration parameters for our DAG: default_args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'mdesafari',  #  owner of the DAG\n",
    "    'depends_on_past': False, # whether a task depends on the success of its previous runs (True) or not (False)\n",
    "    'start_date': datetime(2024, 4, 1),  # start date of the DAG. Tasks will only be scheduled after this date.\n",
    "    'end_date': datetime(2024, 4, 7),  # end date for the DAG\n",
    "    'email': ['your_email@gmail.com'],  # (optional) list of email addresses to receive notifications\n",
    "    'email_on_failure': False, # whether email notifications should be sent on task failure (True) or not (False).\n",
    "    'email_on_retry': False,  # whether email notifications should be sent on task retry after failure (True) or not (False).\n",
    "    'retries': 2,  # max number of retry attempts in case of task failure\n",
    "    'retry_delay': timedelta(minutes=2),  # time interval between retry attempts for failed tasks\n",
    "    'schedule_interval': '0 22 * * *',  # DAG will run at 22:00 (10 PM) UTC every day. (more details below)\n",
    "    'catchup': False  # wether to execute missed previous tasks during the initial scheduling (or after a change in default_args) of the DAG or not.\n",
    "}\n",
    "\n",
    "# cron-like expression '0 22 * * *':\n",
    "# 0: minutes\n",
    "# 22: hour\n",
    "# *: Day of the month and * means all days of the month\n",
    "# *: Month and * means all month\n",
    "# *: Day of the week and * means all days of the week\n",
    "# note: we specified a end_date so the will run while date <= end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. Define our DAG (in the `weather_dag.py` file)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG(\n",
    "    dag_id='weather_dag',\n",
    "    default_args=default_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4. Sign up to OpenWeatherMap to get an API key (needed to access data) + Select an endpoint**\n",
    "\n",
    "> * **Website:** https://openweathermap.org/\n",
    ">\n",
    "> * **Create an account:** click on `Sign in` > Create an account > Follow the instructions\n",
    ">\n",
    "> * **Valide your email address**\n",
    ">\n",
    "> * **Click on your accounr > My API keys** then copy the generated API key\n",
    ">\n",
    "> **Note:** create your own. I will desactivate mine to encourage you to make your own.\n",
    ">\n",
    "> * **Now, click on API on the tool bar** > Scroll down to `Current & Forecast weather data collection`\n",
    ">\n",
    "> * **Click on API doc** > Scroll down to JSON format to see an example of content > \n",
    ">\n",
    "> * **Scroll down again to API call**\n",
    ">\n",
    "> <img src=\"./images/api_call.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/dag_example.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4. Create ours Tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 1: Check if the weather API is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "city_name = 'Paris'\n",
    "endpoint = f'/data/2.5/weather?q={city_name}&appid={api_key}'\n",
    "\n",
    "is_weather_api_ready = HttpSensor(\n",
    "    task_id ='is_weather_api_ready',  # unique identifier for the task\n",
    "    http_conn_id='weathermap_api',  # Airflow connection ID, which contains the configuration details for the HTTP connection to the weather API.\n",
    "    endpoint=endpoint,  # API endpoint path to query weather data for Portland, along with the API key (APPID) needed for authentication.\n",
    "    dag=dag # assigns the task to our DAG named 'dag' (and defined above)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and configure the http_conn_id for task 1 in Airflow to access to OpenWeather API**\n",
    "\n",
    "* **Go to Airflow on your web browser:** localhost:8080 > sign in\n",
    "\n",
    "> * **username:** admin\n",
    "> * **password:** find it in airflow > standalone_admin_password.txt\n",
    "\n",
    "* **Click on Admin** > Connections > Add a new connection\n",
    "\n",
    "> * **Connection id:** `weathermap_api`\n",
    ">\n",
    "> * **Connection type:** `HTTP`\n",
    ">\n",
    "> * **Host:** `https://api.openweathermap.org`\n",
    ">\n",
    "> <img src=\"./images/api_call.png\" width=\"500px\" />\n",
    ">\n",
    "> * **Save**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 2: Extract Weather Data From OpenWeather API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_weather_data = SimpleHttpOperator(  # operator used to make an HTTP request to extract weather data\n",
    "    task_id = 'extract_weather_data',  # unique identifier for this task in the DAG\n",
    "    http_conn_id = 'weathermap_api',  # id of the HTTP connection to be used for the request (we set it in airflow)\n",
    "    endpoint=endpoint,  # URL endpoint to which the HTTP request will be made\n",
    "    method = 'GET',  # The operator will perform a GET request to retrieve weather data\n",
    "    response_filter= lambda r: json.loads(r.text),  # a lambda function that converts the response text into a JSON object\n",
    "    log_response=True,  # nables logging of the response received. Response details will be logged in the Airflow task logs\n",
    "    dag=dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 3: Transform and Load the Weather Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **In the `custom_modules` folder, create a Python file named: `__init__.py`**\n",
    "\n",
    "This will allow airflow to recognize `custom_modules` as a module folder. This will allows us to import functions from this folder. It's mandatory.\n",
    "\n",
    "> Leave content empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **In the `custom_modules` folder, create a Python file named: `transform_load_weather_funcs.py`**\n",
    "\n",
    "Then, create (copy/paste) the following transformation and loading code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import errno\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "def kelvin_to_degree_celsius(temp_in_kelvin):\n",
    "    return temp_in_kelvin - 273.15\n",
    "\n",
    "\n",
    "def transform_load_data(task_instance):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        task_instance: represents the state of the previous task.\n",
    "            It provides access to data of the previous task using XCom (Cross-communication).\n",
    "    \"\"\"\n",
    "    # use xcom_pull to get data of the previous task whose id is 'extract_weather_data'\n",
    "    data = task_instance.xcom_pull(task_ids=\"extract_weather_data\")\n",
    "\n",
    "    # extracts the city name from the data.\n",
    "    city = data[\"name\"]\n",
    "\n",
    "    # extracts the weather description from the data\n",
    "    weather_description = data[\"weather\"][0]['description']\n",
    "\n",
    "    # converts the temperature in Kelvin to degree Celsius\n",
    "    temp_celsius = kelvin_to_degree_celsius(data[\"main\"][\"temp\"])\n",
    "\n",
    "    # converts the \"feels like\" temperature in Kelvin to degree Celsius\n",
    "    feels_like_celsius = kelvin_to_degree_celsius(data[\"main\"][\"feels_like\"])\n",
    "\n",
    "    # converts the minimum temperature in Kelvin to degree Celsius\n",
    "    min_temp_celsius = kelvin_to_degree_celsius(data[\"main\"][\"temp_min\"])\n",
    "\n",
    "    # converts the maximum temperature in Kelvin to degree Celsius\n",
    "    max_temp_celsius = kelvin_to_degree_celsius(data[\"main\"][\"temp_max\"])\n",
    "\n",
    "    # extracts the atmospheric pressure from the data\n",
    "    pressure = data[\"main\"][\"pressure\"]\n",
    "\n",
    "    # extracts the humidity from the data\n",
    "    humidity = data[\"main\"][\"humidity\"]\n",
    "\n",
    "    # extracts the wind speed from the data\n",
    "    wind_speed = data[\"wind\"][\"speed\"]\n",
    "\n",
    "    # calculates the timestamp of the weather record, sunrise time, and sunset time\n",
    "    time_of_record = datetime.fromtimestamp(data['dt'] + data['timezone'])\n",
    "    sunrise_time = datetime.fromtimestamp(data['sys']['sunrise'] + data['timezone'])\n",
    "    sunset_time = datetime.fromtimestamp(data['sys']['sunset'] + data['timezone'])\n",
    "\n",
    "    # creates a dictionary transformed_data containing the transformed data\n",
    "    transformed_data = {\"City\": city,\n",
    "                        \"Description\": weather_description,\n",
    "                        \"Temperature (F)\": temp_celsius,\n",
    "                        \"Feels Like (F)\": feels_like_celsius,\n",
    "                        \"Minimun Temp (F)\":min_temp_celsius,\n",
    "                        \"Maximum Temp (F)\": max_temp_celsius,\n",
    "                        \"Pressure\": pressure,\n",
    "                        \"Humidty\": humidity,\n",
    "                        \"Wind Speed\": wind_speed,\n",
    "                        \"Time of Record\": time_of_record,\n",
    "                        \"Sunrise (Local Time)\":sunrise_time,\n",
    "                        \"Sunset (Local Time)\": sunset_time                        \n",
    "                        }\n",
    "    \n",
    "    # creates a list transformed_data_list containing the transformed data dictionary\n",
    "    transformed_data_list = [transformed_data]\n",
    "\n",
    "    # converts the transformed data list into a pandas DataFrame.\n",
    "    df_data = pd.DataFrame(transformed_data_list)\n",
    "\n",
    "    # get the current date components\n",
    "    now = pd.Timestamp.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "    day = now.day\n",
    "\n",
    "    # save the DataFrame as a CSV file with a dynamically generated filename\n",
    "    # to a local storage (in our airflow directory).\n",
    "    # In /opt/airflow we will create a directrory named 'weather' and a subdirectory with the city's name\n",
    "    data_dir = f'/opt/airflow/weather/{city}'\n",
    "\n",
    "    # create the directory path if it does not exist\n",
    "    try:\n",
    "        os.makedirs(data_dir)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "    \n",
    "    # saving path\n",
    "    path_save = f'{data_dir}/weather_{year}_{month}_{day}.csv'\n",
    "\n",
    "    # save the dataframe (in csv format)\n",
    "    df_data.to_csv(path_save, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Import the function `transform_load_data` in our dag file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_modules.transform_load_weather_funcs import transform_load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create the transform and load task in our dag file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_load_weather_data = PythonOperator(\n",
    "    task_id= 'transform_load_weather_data',\n",
    "    python_callable=transform_load_data,\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define the tasks dependencies (in the dag file)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_weather_api_ready >> extract_weather_data >> transform_load_weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Create our Second DAG: improvement of the first one**\n",
    "\n",
    "<img src=\"./images/improve/new_dag_schema.png\" width=\"700px\" />\n",
    "\n",
    "* **Instead of storing the transformed weather data in the local system, we're going to store it in a PostgreSQL database. To do this, we need to update / modify :**\n",
    "\n",
    "> * dockerfile\n",
    "> * docker-compose.yml\n",
    "> * transform_load_data function\n",
    "\n",
    "* **We also need to create our PostgreSQL database.**\n",
    "\n",
    "> * Database Name: weather\n",
    "\n",
    "* **Then, create a task to create a table (`paris`) if it does not exist**\n",
    "\n",
    "* **Create a Python function to insert data**\n",
    "\n",
    "* **Create a task that execute this insertion function**\n",
    "\n",
    "* **Re-build the dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1. Modifications: dockerfile, docker-compose.yml, and transform_load_data function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **dockerfile**\n",
    "\n",
    "<img src=\"./images/improve/dockerfile.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **docker-compose.yml**\n",
    "\n",
    "<img src=\"./images/improve/docker-compose.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **transform_load_data**\n",
    "\n",
    "> I created a new Python file in `custom_modules` folder named `tl_weather_postgres.py` and its content is the one below:\n",
    ">\n",
    "> > * We have our previous functions: (1) kelvin_to_degree_celsius (2) transform_load_data\n",
    "> > `transform_load_data` has been modified as below\n",
    "> >\n",
    "> > * I also added a new function named `insert_data_to_postgres_db` to insert data into our PostgreSQL database (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import errno\n",
    "from datetime import timedelta, datetime\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "\n",
    "\n",
    "def kelvin_to_degree_celsius(temp_in_kelvin):\n",
    "    return temp_in_kelvin - 273.15\n",
    "\n",
    "# we modified the function name from transform_load_weather_data to transform_weather_data\n",
    "# so, modify also the corresponding dag id and name\n",
    "def transform_data(task_instance):\n",
    "    data = task_instance.xcom_pull(task_ids=\"extract_weather_data\")\n",
    "    city = data[\"name\"]\n",
    "    weather_description = data[\"weather\"][0]['description']\n",
    "    temp_celsius = kelvin_to_degree_celsius(data[\"main\"][\"temp\"])\n",
    "    time_of_record = datetime.fromtimestamp(data['dt'] + data['timezone'])\n",
    "\n",
    "    transformed_data = {\n",
    "        \"city\": city,\n",
    "        \"description\": weather_description,\n",
    "        \"temperature\": temp_celsius,\n",
    "        \"time\": time_of_record \n",
    "    }\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "# here also, the name of the previous dag is transform_weather_data instead of transform_load_weather_data\n",
    "def insert_data_to_postgres_db(task_instance):\n",
    "    transformed_data = task_instance.xcom_pull(task_ids='transform_weather_data')\n",
    "    insert_sql = \"INSERT INTO paris (city, description, temperature, time) VALUES (%s, %s, %s, %s)\"\n",
    "    pg_hook = PostgresHook(postgres_conn_id='postgres_id')\n",
    "    pg_hook.run(\n",
    "        insert_sql,\n",
    "        parameters=(\n",
    "            transformed_data['city'],\n",
    "            transformed_data['description'],\n",
    "            transformed_data['temperature'],\n",
    "            transformed_data['time']\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2. Create our PostgreSQL database named `weather`**\n",
    "\n",
    "* Open pg Admin\n",
    "* Authentify (your password)\n",
    "* Create the db: `weather`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2. Create a task to create a table (`paris`) if it does not exist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PostgresOperator\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "\n",
    "# also, do not forget to update path to our custom functions\n",
    "# tl_weather_postgres instead of transform_load_weather_funcs (as in the previous video)\n",
    "from custom_modules.tl_weather_postgres import transform_load_data, insert_data_to_postgres_db\n",
    "\n",
    "\n",
    "create_weather_table = PostgresOperator(\n",
    "    task_id='create_weather_table',\n",
    "    postgres_conn_id='postgres_id',  # id of our postgre connector that we will create once airflow is launched\n",
    "    sql='''CREATE TABLE IF NOT EXISTS paris (\n",
    "        city VARCHAR(255),\n",
    "        description VARCHAR(255),\n",
    "        temperature FLOAT,\n",
    "        time TIMESTAMP\n",
    "    );''',\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.3. Create a Python function to insert data**\n",
    "\n",
    "This have already been done when modifying `transform_load_data` function. See `section 5.1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.4. Create a task that execute this insertion function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert data in the PostgreSQL db named `weather`, in its table named `paris` (task)\n",
    "insert_data_to_postgres = PythonOperator(\n",
    "    task_id='insert_data_to_postgres',\n",
    "    python_callable=insert_data_to_postgres_db,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.5. Re-build the dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_weather_data is executed after is_weather_api_ready\n",
    "# transform_load_weather_data is executed after extract_weather_data\n",
    "is_weather_api_ready >> extract_weather_data >> transform_weather_data\n",
    "\n",
    "# to execute create_weather_table, is_weather_api_ready must be completed\n",
    "create_weather_table << is_weather_api_ready\n",
    "\n",
    "# to execute insert_data_to_postgres, transform_load_weather_data and create_weather_table \n",
    "# must be completed\n",
    "insert_data_to_postgres << transform_weather_data\n",
    "insert_data_to_postgres << create_weather_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/improve/new_dag.png\" width=\"700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Here is the complete dag content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from datetime import timedelta, datetime\n",
    "from airflow.providers.http.sensors.http import HttpSensor\n",
    "import json\n",
    "from airflow.providers.http.operators.http import SimpleHttpOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from custom_modules.tl_weather_postgres import transform_data, insert_data_to_postgres_db\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'mdesafari',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 4, 1),\n",
    "    'end_date': datetime(2024, 4, 7),\n",
    "    'email': ['your_email@gmail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=2),\n",
    "    'schedule_interval': '0 22 * * *',\n",
    "    'catchup': False\n",
    "}\n",
    "\n",
    "# 1. CREATE THE DAG DEFINITION\n",
    "dag = DAG(\n",
    "    dag_id='weather_dag_postgres',\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "# 2. CREATE TASKS\n",
    "# 2.1. Task 1: Check if the weather API is ready\n",
    "api_key = ''\n",
    "city_name = 'Paris'\n",
    "endpoint = f'/data/2.5/weather?q={city_name}&appid={api_key}'\n",
    "\n",
    "is_weather_api_ready = HttpSensor(\n",
    "    task_id ='is_weather_api_ready',\n",
    "    http_conn_id='weathermap_api',\n",
    "    endpoint=endpoint,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# 2.2. Task 2: Extract weather data from the API\n",
    "extract_weather_data = SimpleHttpOperator(\n",
    "    task_id = 'extract_weather_data',\n",
    "    http_conn_id = 'weathermap_api',\n",
    "    endpoint=endpoint,\n",
    "    method = 'GET',\n",
    "    response_filter= lambda r: json.loads(r.text),\n",
    "    log_response=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# 2.3. Task 3: transform and load weather data\n",
    "transform_weather_data = PythonOperator(\n",
    "    task_id= 'transform_weather_data',\n",
    "    python_callable=transform_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "# 3. creata postgres tasks\n",
    "# 3.1. create table\n",
    "create_weather_table = PostgresOperator(\n",
    "    task_id='create_weather_table',\n",
    "    postgres_conn_id='postgres_id',\n",
    "    sql='''CREATE TABLE IF NOT EXISTS paris (\n",
    "        city VARCHAR(255),\n",
    "        description VARCHAR(255),\n",
    "        temperature FLOAT,\n",
    "        time TIMESTAMP\n",
    "    );''',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "# 3.2. insert data\n",
    "insert_data_to_postgres = PythonOperator(\n",
    "    task_id='insert_data_to_postgres',\n",
    "    python_callable=insert_data_to_postgres_db,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "# 4. TASKS DEPENDENCIES\n",
    "is_weather_api_ready >> extract_weather_data >> transform_weather_data\n",
    "create_weather_table << is_weather_api_ready\n",
    "insert_data_to_postgres << transform_weather_data\n",
    "insert_data_to_postgres << create_weather_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Re-build the image (because dockerfile updated) before Compose Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.6. Execution**\n",
    "\n",
    "> * **Create the PostegreSQL connection**\n",
    ">\n",
    "> > * **Connection Id:** `postgres_id`\n",
    "> >\n",
    "> > * **Connection Type:** `Postgres`\n",
    "> >\n",
    "> > * **Host:** `host.docker.internal`\n",
    "> >\n",
    "> > * **Database:** `weather`\n",
    "> >\n",
    "> > * **Login:** `postgres`\n",
    "> >\n",
    "> > * **Password:** `your_postgres_db_password_here`\n",
    "> >\n",
    "> > * **Port:** `5432`\n",
    ">\n",
    "> * **You will need to re-create the connection to OpenWeatherMap API since we built a new Image and Composing Up wich changes our initial configuration**\n",
    ">\n",
    "> > * **Connection Id:** `weathermap_api`\n",
    "> >\n",
    "> > * **Connection Type:** `HTTP`\n",
    "> >\n",
    "> > * **Host:** `https://api.openweathermap.org`\n",
    ">\n",
    "> * **Then Compose Down before Composing Up again**\n",
    ">\n",
    "> * **Trigger manually the DAG to see if it works + Check if your PostgreSQL db (`weather`) got the data**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
